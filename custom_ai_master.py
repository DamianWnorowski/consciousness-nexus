#!/usr/bin/env python3
"""
üß† CUSTOM AI MASTER - Comprehensive AI Endpoint Management
==========================================================

Multi-endpoint orchestration, streaming, self-learning, and optimization.
"""

import asyncio
import json
import time
import random
import os
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import argparse
import aiohttp

class EndpointType(Enum):
    OPENAI_COMPATIBLE = "openai_compatible"
    ANTHROPIC_BATCH = "anthropic_batch"
    OLLAMA_COMPATIBLE = "ollama_compatible"
    CUSTOM_HTTP = "custom_http"
    WEBSOCKET = "websocket"
    ENTERPRISE_API = "enterprise_api"

class RouterStrategy(Enum):
    WEIGHTED_ENSEMBLE = "weighted_ensemble"
    CASCADE = "cascade"
    CAPABILITY_BASED = "capability_based"
    LOAD_BALANCED = "load_balanced"

@dataclass
class EndpointConfig:
    """Configuration for a single AI endpoint"""
    name: str
    type: EndpointType
    url: str
    api_key_env: Optional[str] = None
    weight: float = 1.0
    capabilities: List[str] = field(default_factory=list)
    streaming: Dict[str, Any] = field(default_factory=dict)
    health_check_url: Optional[str] = None
    timeout: int = 30
    retry_count: int = 3

@dataclass
class EndpointMetrics:
    """Performance metrics for an endpoint"""
    response_time: float = 0.0
    success_rate: float = 1.0
    total_requests: int = 0
    error_count: int = 0
    last_health_check: float = 0.0
    is_healthy: bool = True

@dataclass
class QueryResult:
    """Result from an AI endpoint query"""
    endpoint_name: str
    response: str
    confidence: float
    tokens_used: int = 0
    response_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)

class CustomAIMaster:
    """Master controller for custom AI endpoints"""

    def __init__(self, config_file: str = "settings.local.json"):
        self.config_file = config_file
        self.endpoints: Dict[str, EndpointConfig] = {}
        self.metrics: Dict[str, EndpointMetrics] = {}
        self.session: Optional[aiohttp.ClientSession] = None

        # Load configuration
        self.load_config()

        # Initialize metrics for all endpoints
        for name in self.endpoints:
            self.metrics[name] = EndpointMetrics()

    def load_config(self):
        """Load endpoint configuration"""
        try:
            with open(self.config_file, 'r') as f:
                config = json.load(f)

            ai_config = config.get('custom_ai_endpoints', {})
            if not ai_config.get('enabled', False):
                print("‚ùå Custom AI endpoints not enabled in config")
                return

            # Load endpoints
            endpoints_config = ai_config.get('endpoints', {})
            for name, endpoint_data in endpoints_config.items():
                endpoint = EndpointConfig(
                    name=name,
                    type=EndpointType(endpoint_data['type']),
                    url=endpoint_data['url'],
                    api_key_env=endpoint_data.get('api_key_env'),
                    weight=endpoint_data.get('weight', 1.0),
                    capabilities=endpoint_data.get('capabilities', []),
                    streaming=endpoint_data.get('streaming', {}),
                    health_check_url=endpoint_data.get('health_check_url'),
                    timeout=endpoint_data.get('timeout', 30),
                    retry_count=endpoint_data.get('retry_count', 3)
                )
                self.endpoints[name] = endpoint

            print(f"[+] Loaded {len(self.endpoints)} AI endpoints")

        except FileNotFoundError:
            print(f"[!] Config file {self.config_file} not found, using defaults")
            self.create_default_config()
        except Exception as e:
            print(f"‚ùå Error loading config: {e}")

    def create_default_config(self):
        """Create default configuration"""
        default_config = {
            "custom_ai_endpoints": {
                "enabled": True,
                "router": "weighted_ensemble",
                "streaming": {"enabled": True, "bidirectional": True},
                "self_learning": {"enabled": True, "auto_heal": True},
                "ml_optimization": {"enabled": True, "offline_mode": True},
                "endpoints": {
                    "local_ollama": {
                        "type": "ollama_compatible",
                        "url": "http://localhost:11434/api/chat",
                        "weight": 0.8,
                        "capabilities": ["chat", "coding", "analysis"],
                        "streaming": {"enabled": True, "type": "http_streaming"}
                    },
                    "openai_fallback": {
                        "type": "openai_compatible",
                        "url": "https://api.openai.com/v1/chat/completions",
                        "api_key_env": "OPENAI_API_KEY",
                        "weight": 0.6,
                        "capabilities": ["chat", "coding", "creative"],
                        "streaming": {"enabled": True, "type": "http_streaming"}
                    }
                }
            }
        }

        # Create directory if needed
        config_dir = os.path.dirname(self.config_file)
        if config_dir:
            os.makedirs(config_dir, exist_ok=True)
        with open(self.config_file, 'w') as f:
            json.dump(default_config, f, indent=2)

        print(f"[+] Created default config at {self.config_file}")

    async def initialize(self):
        """Initialize the AI master"""
        self.session = aiohttp.ClientSession()
        print("[*] Custom AI Master initialized")

    async def shutdown(self):
        """Shutdown the AI master"""
        if self.session:
            await self.session.close()
        print("[*] Custom AI Master shutdown")

    def select_endpoints(self, capability: str, count: int = 1) -> List[str]:
        """Select best endpoints for a capability"""
        candidates = []

        for name, endpoint in self.endpoints.items():
            if capability in endpoint.capabilities:
                metrics = self.metrics[name]
                # Calculate score based on weight and health
                score = endpoint.weight * (1.0 if metrics.is_healthy else 0.1)
                candidates.append((name, score))

        # Sort by score and return top candidates
        candidates.sort(key=lambda x: x[1], reverse=True)
        return [name for name, _ in candidates[:count]]

    async def query_endpoint(self, endpoint_name: str, prompt: str,
                           capability: str = "chat") -> Optional[QueryResult]:
        """Query a specific endpoint"""
        if endpoint_name not in self.endpoints:
            return None

        endpoint = self.endpoints[endpoint_name]
        metrics = self.metrics[endpoint_name]

        start_time = time.time()

        try:
            # Prepare request based on endpoint type
            request_data = self.prepare_request(endpoint, prompt, capability)

            # Make request with retries
            for attempt in range(endpoint.retry_count + 1):
                try:
                    async with self.session.post(
                        endpoint.url,
                        json=request_data,
                        headers=self.get_headers(endpoint),
                        timeout=aiohttp.ClientTimeout(total=endpoint.timeout)
                    ) as response:
                        if response.status == 200:
                            result_data = await response.json()
                            response_text = self.extract_response(result_data, endpoint)

                            # Update metrics
                            response_time = time.time() - start_time
                            metrics.response_time = response_time
                            metrics.total_requests += 1
                            metrics.success_rate = (metrics.total_requests - metrics.error_count) / metrics.total_requests

                            return QueryResult(
                                endpoint_name=endpoint_name,
                                response=response_text,
                                confidence=random.uniform(0.7, 0.95),  # Mock confidence
                                response_time=response_time,
                                metadata={"status_code": response.status, "attempt": attempt + 1}
                            )
                        else:
                            print(f"‚ö†Ô∏è Attempt {attempt + 1} failed with status {response.status}")

                except Exception as e:
                    print(f"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}")

                if attempt < endpoint.retry_count:
                    await asyncio.sleep(1)  # Wait before retry

            # All attempts failed
            metrics.error_count += 1
            metrics.success_rate = (metrics.total_requests - metrics.error_count) / metrics.total_requests
            return None

        except Exception as e:
            print(f"[-] Error querying {endpoint_name}: {e}")
            metrics.error_count += 1
            return None

    def prepare_request(self, endpoint: EndpointConfig, prompt: str, capability: str) -> Dict[str, Any]:
        """Prepare request data based on endpoint type"""
        if endpoint.type == EndpointType.OLLAMA_COMPATIBLE:
            return {
                "model": "llama2",  # Default model
                "prompt": prompt,
                "stream": False
            }
        elif endpoint.type == EndpointType.OPENAI_COMPATIBLE:
            return {
                "model": "gpt-3.5-turbo",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 1000,
                "temperature": 0.7
            }
        else:
            # Generic format
            return {
                "prompt": prompt,
                "capability": capability,
                "max_tokens": 1000
            }

    def get_headers(self, endpoint: EndpointConfig) -> Dict[str, str]:
        """Get headers for endpoint request"""
        headers = {"Content-Type": "application/json"}

        if endpoint.api_key_env:
            api_key = os.getenv(endpoint.api_key_env)
            if api_key:
                if endpoint.type == EndpointType.OPENAI_COMPATIBLE:
                    headers["Authorization"] = f"Bearer {api_key}"
                else:
                    headers["Authorization"] = api_key

        return headers

    def extract_response(self, response_data: Dict[str, Any], endpoint: EndpointConfig) -> str:
        """Extract response text from API response"""
        if endpoint.type == EndpointType.OLLAMA_COMPATIBLE:
            return response_data.get("response", "")
        elif endpoint.type == EndpointType.OPENAI_COMPATIBLE:
            choices = response_data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content", "")
        else:
            return response_data.get("response", str(response_data))

        return "No response generated"

    async def query(self, prompt: str, capability: str = "chat",
                   endpoints: Optional[List[str]] = None) -> List[QueryResult]:
        """Query multiple endpoints and return results"""
        if not endpoints:
            endpoints = self.select_endpoints(capability, count=3)

        if not endpoints:
            print(f"[-] No endpoints available for capability: {capability}")
            return []

        print(f"[*] Querying {len(endpoints)} endpoints for '{capability}': {', '.join(endpoints)}")

        # Query all endpoints concurrently
        tasks = [self.query_endpoint(name, prompt, capability) for name in endpoints]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Filter out None results and exceptions
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"‚ùå Endpoint {endpoints[i]} failed with exception: {result}")
            elif result is not None:
                valid_results.append(result)

        return valid_results

    def show_status(self):
        """Show status of all endpoints"""
        print("[*] CUSTOM AI ENDPOINTS STATUS")
        print("=" * 50)

        for name, endpoint in self.endpoints.items():
            metrics = self.metrics[name]
            status = "[+] HEALTHY" if metrics.is_healthy else "[-] UNHEALTHY"

            print(f"\n[*] {name} ({endpoint.type.value})")
            print(f"   Status: {status}")
            print(f"   URL: {endpoint.url}")
            print(f"   Capabilities: {', '.join(endpoint.capabilities)}")
            print(f"   Weight: {endpoint.weight}")
            print(".2f")
            print(".1f")
            print(f"   Total Requests: {metrics.total_requests}")

    async def health_check_all(self):
        """Perform health checks on all endpoints"""
        print("[*] Performing health checks...")

        for name, endpoint in self.endpoints.items():
            is_healthy = await self.health_check_endpoint(name)
            self.metrics[name].is_healthy = is_healthy
            self.metrics[name].last_health_check = time.time()

        healthy_count = sum(1 for m in self.metrics.values() if m.is_healthy)
        print(f"[+] Health check complete: {healthy_count}/{len(self.endpoints)} endpoints healthy")

    async def health_check_endpoint(self, endpoint_name: str) -> bool:
        """Health check a specific endpoint"""
        if endpoint_name not in self.endpoints:
            return False

        endpoint = self.endpoints[endpoint_name]

        try:
            # Simple health check - try to get a basic response
            async with self.session.get(
                endpoint.health_check_url or endpoint.url,
                timeout=aiohttp.ClientTimeout(total=5)
            ) as response:
                return response.status in [200, 201, 202]

        except Exception:
            return False

async def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="Custom AI Master Controller")
    parser.add_argument("--query", help="Query to send to AI endpoints")
    parser.add_argument("--capability", default="chat", help="Required capability")
    parser.add_argument("--endpoints", nargs="*", help="Specific endpoints to use")
    parser.add_argument("--status", action="store_true", help="Show endpoint status")
    parser.add_argument("--health-check", action="store_true", help="Perform health checks")
    parser.add_argument("--config", default="settings.local.json", help="Config file path")

    args = parser.parse_args()

    # Initialize AI master
    master = CustomAIMaster(args.config)
    await master.initialize()

    try:
        if args.status:
            master.show_status()
        elif args.health_check:
            await master.health_check_all()
            master.show_status()
        elif args.query:
            # Query endpoints
            results = await master.query(args.query, args.capability, args.endpoints)

            print(f"\n[*] QUERY RESULTS: '{args.query}'")
            print("=" * 50)

            for result in results:
                print(f"\n[*] {result.endpoint_name}")
                print(".2f")
                print(f"   Tokens: {result.tokens_used}")
                print(f"   Response: {result.response[:200]}..." if len(result.response) > 200 else f"   Response: {result.response}")

        else:
            parser.print_help()

    finally:
        await master.shutdown()

if __name__ == "__main__":
    asyncio.run(main())
